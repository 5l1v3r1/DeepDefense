{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fJk7zmMjcFm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MOKVbdzBjlmP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Bidirectional\n",
    "from keras.utils import plot_model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# number_of_samples determine how many samples from the attack and normal dataset should be read and used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u7H2jNs1csN2"
   },
   "outputs": [],
   "source": [
    "number_of_samples = 300000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data from attack and normal datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WkSbWhYPjpdP"
   },
   "outputs": [],
   "source": [
    "data_attack = pd.read_csv('../../dataset/dataset_attack.csv', nrows = number_of_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MlnVmtsDj0Af"
   },
   "outputs": [],
   "source": [
    "data_normal = pd.read_csv('../../dataset/dataset_normal.csv', nrows = number_of_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kMoOG1gDj1eJ"
   },
   "outputs": [],
   "source": [
    "data_normal.columns=[ 'frame.len', 'frame.protocols', 'ip.hdr_len',\n",
    "       'ip.len', 'ip.flags.rb', 'ip.flags.df', 'p.flags.mf', 'ip.frag_offset',\n",
    "       'ip.ttl', 'ip.proto', 'ip.src', 'ip.dst', 'tcp.srcport', 'tcp.dstport',\n",
    "       'tcp.len', 'tcp.ack', 'tcp.flags.res', 'tcp.flags.ns', 'tcp.flags.cwr',\n",
    "       'tcp.flags.ecn', 'tcp.flags.urg', 'tcp.flags.ack', 'tcp.flags.push',\n",
    "       'tcp.flags.reset', 'tcp.flags.syn', 'tcp.flags.fin', 'tcp.window_size',\n",
    "       'tcp.time_delta','class']\n",
    "data_attack.columns=[ 'frame.len', 'frame.protocols', 'ip.hdr_len',\n",
    "       'ip.len', 'ip.flags.rb', 'ip.flags.df', 'p.flags.mf', 'ip.frag_offset',\n",
    "       'ip.ttl', 'ip.proto', 'ip.src', 'ip.dst', 'tcp.srcport', 'tcp.dstport',\n",
    "       'tcp.len', 'tcp.ack', 'tcp.flags.res', 'tcp.flags.ns', 'tcp.flags.cwr',\n",
    "       'tcp.flags.ecn', 'tcp.flags.urg', 'tcp.flags.ack', 'tcp.flags.push',\n",
    "       'tcp.flags.reset', 'tcp.flags.syn', 'tcp.flags.fin', 'tcp.window_size',\n",
    "       'tcp.time_delta','class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop unwanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "13n5FKSJj4Fn"
   },
   "outputs": [],
   "source": [
    "data_normal=data_normal.drop(['ip.src', 'ip.dst','frame.protocols'],axis=1)\n",
    "data_attack=data_attack.drop(['ip.src', 'ip.dst','frame.protocols'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EdgboyqDj-AJ"
   },
   "outputs": [],
   "source": [
    "features=[ 'frame.len', 'ip.hdr_len',\n",
    "       'ip.len', 'ip.flags.rb', 'ip.flags.df', 'p.flags.mf', 'ip.frag_offset',\n",
    "       'ip.ttl', 'ip.proto', 'tcp.srcport', 'tcp.dstport',\n",
    "       'tcp.len', 'tcp.ack', 'tcp.flags.res', 'tcp.flags.ns', 'tcp.flags.cwr',\n",
    "       'tcp.flags.ecn', 'tcp.flags.urg', 'tcp.flags.ack', 'tcp.flags.push',\n",
    "       'tcp.flags.reset', 'tcp.flags.syn', 'tcp.flags.fin', 'tcp.window_size',\n",
    "       'tcp.time_delta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7IxNpgX5kAMg"
   },
   "outputs": [],
   "source": [
    "X_normal= data_normal[features].values\n",
    "X_attack= data_attack[features].values\n",
    "Y_normal= data_normal['class']\n",
    "Y_attack= data_attack['class']\n",
    "X=np.concatenate((X_normal,X_attack))\n",
    "Y=np.concatenate((Y_normal,Y_attack))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b3xIr2rRkWMX"
   },
   "outputs": [],
   "source": [
    "scalar = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "scalar.fit(X)\n",
    "X = scalar.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the class field, replace value 'attack' with 0 and 'normal' with 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0U1VeOtEkYBX"
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(Y)):\n",
    "  if Y[i] ==\"attack\":\n",
    "    Y[i]=0\n",
    "  else:\n",
    "    Y[i]=1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After feature transformation, we get a ùëö√óùëõ' matrix, where ùëö indicates the number of packets and ùëõ' indicates the number of new features after transformation. In order to learn patterns in both long and short term, we use a sliding window to separate continuous packets and reshape the data into a series of time windows with window size ùëá. The label ùë¶ in each window illustrates the last packet. After reshaping, we have a three-dimensional matrix with shape (ùëö‚àíùëá )√óùëá √óùëõ'. Figure illustrates the workflow of feature extraction, transformation, and reshaping.\n",
    "![dataset transformation](feature_extraction.png)\n",
    "In this way, we change the features from conventional packet-based to window-based, by which we can learn network patterns from both previous (ùëá ‚àí1) packets and current packet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PEK_22RTkrvH"
   },
   "outputs": [],
   "source": [
    "features = len(X[0])\n",
    "samples = X.shape[0]\n",
    "train_len = 25\n",
    "input_len = samples - train_len\n",
    "I = np.zeros((samples - train_len, train_len, features))\n",
    "\n",
    "for i in range(input_len):\n",
    "    temp = np.zeros((train_len, features))\n",
    "    for j in range(i, i + train_len - 1):\n",
    "        temp[j-i] = X[j]\n",
    "    I[i] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600000, 25)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1l5ft9AblBln"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(I, Y[25:600000], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture used is \n",
    "![Model](model_brnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SzHoR1ewdODm"
   },
   "outputs": [],
   "source": [
    "def create_baseline():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(64, activation='tanh', kernel_regularizer='l2')))\n",
    "    model.add(Dense(128, activation = 'relu', kernel_regularizer='l2'))\n",
    "    model.add(Dense(1, activation = 'sigmoid', kernel_regularizer='l2'))\n",
    "    \n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fW-zMrYFdVP9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/research/btechs813/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = create_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d6nUrwGNdYUP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/research/btechs813/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 383984 samples, validate on 95996 samples\n",
      "Epoch 1/40\n",
      "383984/383984 [==============================] - 323s 841us/step - loss: 0.2111 - acc: 0.9439 - val_loss: 0.1705 - val_acc: 0.9537\n",
      "Epoch 2/40\n",
      "383984/383984 [==============================] - 322s 838us/step - loss: 0.1648 - acc: 0.9548 - val_loss: 0.1711 - val_acc: 0.9542\n",
      "Epoch 3/40\n",
      "383984/383984 [==============================] - 322s 839us/step - loss: 0.1571 - acc: 0.9569 - val_loss: 0.1579 - val_acc: 0.9539\n",
      "Epoch 4/40\n",
      "383984/383984 [==============================] - 322s 838us/step - loss: 0.1501 - acc: 0.9591 - val_loss: 0.1495 - val_acc: 0.9613\n",
      "Epoch 5/40\n",
      "383984/383984 [==============================] - 322s 839us/step - loss: 0.1446 - acc: 0.9605 - val_loss: 0.1477 - val_acc: 0.9589\n",
      "Epoch 6/40\n",
      "383984/383984 [==============================] - 321s 837us/step - loss: 0.1395 - acc: 0.9616 - val_loss: 0.1413 - val_acc: 0.9615\n",
      "Epoch 7/40\n",
      "383984/383984 [==============================] - 322s 837us/step - loss: 0.1374 - acc: 0.9624 - val_loss: 0.1360 - val_acc: 0.9612\n",
      "Epoch 8/40\n",
      "383984/383984 [==============================] - 322s 839us/step - loss: 0.1339 - acc: 0.9636 - val_loss: 0.1317 - val_acc: 0.9646\n",
      "Epoch 9/40\n",
      "383984/383984 [==============================] - 323s 842us/step - loss: 0.1308 - acc: 0.9643 - val_loss: 0.1268 - val_acc: 0.9664\n",
      "Epoch 10/40\n",
      "383984/383984 [==============================] - 321s 836us/step - loss: 0.1261 - acc: 0.9660 - val_loss: 0.1243 - val_acc: 0.9670\n",
      "Epoch 11/40\n",
      "383984/383984 [==============================] - 297s 773us/step - loss: 0.1234 - acc: 0.9670 - val_loss: 0.1194 - val_acc: 0.9696\n",
      "Epoch 12/40\n",
      "383984/383984 [==============================] - 300s 781us/step - loss: 0.1210 - acc: 0.9677 - val_loss: 0.1159 - val_acc: 0.9691\n",
      "Epoch 13/40\n",
      "383984/383984 [==============================] - 300s 780us/step - loss: 0.1169 - acc: 0.9691 - val_loss: 0.1138 - val_acc: 0.9693\n",
      "Epoch 14/40\n",
      "383984/383984 [==============================] - 301s 784us/step - loss: 0.1155 - acc: 0.9695 - val_loss: 0.1127 - val_acc: 0.9702\n",
      "Epoch 15/40\n",
      "383984/383984 [==============================] - 301s 783us/step - loss: 0.1141 - acc: 0.9706 - val_loss: 0.1151 - val_acc: 0.9698\n",
      "Epoch 16/40\n",
      "383984/383984 [==============================] - 301s 785us/step - loss: 0.1105 - acc: 0.9713 - val_loss: 0.1166 - val_acc: 0.9716\n",
      "Epoch 17/40\n",
      "383984/383984 [==============================] - 301s 784us/step - loss: 0.1120 - acc: 0.9716 - val_loss: 0.1124 - val_acc: 0.9701\n",
      "Epoch 18/40\n",
      "383984/383984 [==============================] - 301s 784us/step - loss: 0.1091 - acc: 0.9724 - val_loss: 0.1071 - val_acc: 0.9723\n",
      "Epoch 19/40\n",
      "383984/383984 [==============================] - 300s 782us/step - loss: 0.1070 - acc: 0.9729 - val_loss: 0.1093 - val_acc: 0.9724\n",
      "Epoch 20/40\n",
      "238560/383984 [=================>............] - ETA: 1:48 - loss: 0.1054 - acc: 0.9738"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, epochs = 40,validation_split=0.2, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtained plot of accuracy\n",
    "\n",
    "![plot of accuracy](BRNN_Model_Accuracy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7dR8MHphk68X"
   },
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('BRNN Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='lower right')\n",
    "plt.savefig('BRNN Model Accuracy.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of loss\n",
    "![plot of loss](BRNN_Model_Loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eKjMDYIOk72N"
   },
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('BRNN Model  Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.savefig('BRNN Model Loss.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "heMaPerEdn3c"
   },
   "outputs": [],
   "source": [
    "predict = model.predict(X_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Calculate True positive,True negetive,False positive and False negetive values. Create Heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rAjFgVm6uGGp"
   },
   "outputs": [],
   "source": [
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "predictn = predict.flatten().round()\n",
    "predictn = predictn.tolist()\n",
    "Y_testn = Y_test.tolist()\n",
    "for i in range(len(Y_testn)):\n",
    "  if predictn[i]==1 and Y_testn[i]==1:\n",
    "    tp+=1\n",
    "  elif predictn[i]==0 and Y_testn[i]==0:\n",
    "    tn+=1\n",
    "  elif predictn[i]==0 and Y_testn[i]==1:\n",
    "    fp+=1\n",
    "  elif predictn[i]==1 and Y_testn[i]==0:\n",
    "    fn+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 274
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 949,
     "status": "ok",
     "timestamp": 1557862375571,
     "user": {
      "displayName": "Mohammed Musthafa",
      "photoUrl": "",
      "userId": "16090542423726450957"
     },
     "user_tz": -330
    },
    "id": "0Ad719yjxPL2",
    "outputId": "972ee841-76cd-49f9-f0c7-6961170aae8c"
   },
   "outputs": [],
   "source": [
    "to_heat_map =[[tn,fp],[fn,tp]]\n",
    "to_heat_map = pd.DataFrame(to_heat_map, index = [\"Attack\",\"Normal\"],columns = [\"Attack\",\"Normal\"])\n",
    "ax = sns.heatmap(to_heat_map,annot=True, fmt=\"d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save details "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rMBI_fKCzQBN"
   },
   "outputs": [],
   "source": [
    "figure = ax.get_figure()    \n",
    "figure.savefig('confusion_matrix_BRNN.png', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mdcKrPWEfE44"
   },
   "outputs": [],
   "source": [
    "model.save('brnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mv1vi3_KfPHE"
   },
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "brnn_classifier",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
